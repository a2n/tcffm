--- Log opened 四 10月 30 00:00:34 2014
13:20 < guildwar> 要跑平行程式嗎? 若是要只要方便管理，建議直接用 rocks，還有其他選擇，但我只用過 rocks 就是
13:20 < guildwar> http://www.rocksclusters.org/wordpress/
13:24 < guildwar> 基本上設定好第一台 frontend node, 可再額外設幾台 storage nodes, 其他的都當做 compute nodes
13:29 < guildwar> 當然要自己從頭搞也可以，不過就工程浩大了
13:32 < guildwar> 不然用 drbl server 也行，不過要手動的地方比較多
13:50 < guildwar> 這裡有篇鳥哥用 centos 6.3 和 drbl 架設的 pc-cluster 可參考看看
13:50 < guildwar> http://linux.vbird.org/somepaper/20130521-build_pc_cluster_using_drbl_in_centos_6.3.pdf
14:53 <@haroldwu> 好
15:01 <@haroldwu> 我來研究看看，就是不知道 cluster 建起來以後，計算任務也要能分散到各個 node 才行啊 XDD
15:08 < guildwar> 那是你 coding 的問題，程式裡要改寫成呼叫 mpi 或 cuda 之類的，才會在跑時同時由多台一起跑
15:11 < guildwar> 若是程式不想改，那只能改用 ssi 的方式，不過通常這種方式不能太多台，可能頂多４台而已
15:11 < guildwar> http://kerrighed.org/wiki/index.php/Main_Page
15:41 < guildwar> 如果沒寫過 mpi 程式，這個應該可以參考 http://www.linux-mag.com/id/5759/
15:42 < guildwar> http://www.linux-mag.com/id/4609/
17:40 < dclee9> 今天的TC4H會參加台中前端社群定時聚會
17:40 < dclee9> 在夢種子
17:41 < dclee9> 所以如果有興趣的可以參加
17:41 < dclee9> 他們是講師型的聚會。
19:13 < hmchen> haroldwu 或許是要找 job scheduling 工具, http://en.wikipedia.org/wiki/Portable_Batch_System
19:15 < hmchen> 有時候只是有大量的程式要跑, 但不一定是平行運算, 我之前念研究所跑實驗數據, 是執行同一個程式但需要重複跑30次, 因為要求統計數據
19:19 < hmchen> 當時所裡有一套 PC cluster, 16 rocks x dual xeon, 上面安裝了一套叫 PBSPRo 的管理程式, 其中一個 node 跑 NFS server 掛 /home folder, 或者甚至是 root over NFS 開機, 有點忘了
19:21 < hmchen> 然後寫一個簡單的 shell script, 設定執行擋路徑, 需要幾個 nodes, 然後丟給 PBS 管理, 只要任何 compute node 有空, 就會把 job 自動丟進去跑
19:23 < hmchen> PBS Pro 要錢, 後來有自己試著用 Sun Grid Engine 架出相同功能的環境, 不是 open source 但不用錢, 現在變成 Oracle Grid Engine 了
19:24 < hmchen> 剛剛找一下,現在有類似的 projects, 有 Torque 跟 open grid engine
19:29 < hmchen> 如果是要跑單一任務的平行運算程式, 這年頭用 PC cluster + MPI 架構不太划算, 改學 cuda 或 OpenCL 搭配顯示卡 GPU, 說不定程式開發跟實際執行速度還比較快
19:31 < hmchen> 網路版的MPI平行化程式光是要除錯就會出人命了, cuda + gpu 起碼還是在單機上跑, 有 tool 可以即時除錯, 監看變數或中斷點之類的
19:36 < guildwar> 可是一開始就說硬體是 10 台機器囉，當然用 mpi 最合用
19:38 < guildwar> 學生不大可能去買一張幾萬元的 GPU，然後買 10 張
19:53 < hmchen> 還是看用途啦, 平行計算當然是 mpi 或 cuda, 多重任務程式就靠 pbs 這類的
19:55 < hmchen> 但說真的 GPU 比 MPI 強大多了, 硬體成本也比較便宜, 中階的卡其實就可以達到 4x~10x 不等的加速效果, 看計算架構而定
19:55 < hmchen> 因為 MPI 平行計算的瓶頸, 其實是卡在 data 的分散與再蒐集
19:56 < hmchen> node 之間得靠高速網路連接減少資料交換的等待時間
19:57 < hmchen> 以前沒有 gigabit ethernet 時, 還得用特殊網路卡
19:57 < hmchen> 窮人的 cluster 則是用多網卡 + channel bounding 倍增流量
19:57 < guildwar> 有看到這個，若 nvidia GPU 在機器上，就可以用 CUDA-Aware MPI
19:57 < guildwar> http://devblogs.nvidia.com/parallelforall/introduction-cuda-aware-mpi/
19:57 < hmchen> 但 GPU 跟 CPU 之間的頻寬遠大於網路卡
19:58 < guildwar> 畢竟要用 opencl 應該難度還是滿高的吧，光 porting code 就很慘了
19:59 < hmchen> cuda 語法比 opencl 簡單
19:59 < hmchen> 可惜就是 nvidia 限定
20:00 < hmchen> opencl 則可以 cpu + gpu 同時混合執行
20:00 < guildwar> 對啊，所以用 mpi 比較快上手
20:00 < guildwar> How to Build a GPU-Accelerated Research Cluster
20:00 < guildwar> http://devblogs.nvidia.com/parallelforall/how-build-gpu-accelerated-research-cluster/
20:00 < guildwar> Tune GPU and CPU load balancing for your application
20:01 < hmchen> 這個是把 cuda/opencl 包裝成 mpi 的架構去寫程式嗎?
20:01 < guildwar> 這個 cuda-aware mpi 好像就會自動分配 cpu 及 gpu
20:01 < guildwar> 我不懂，剛剛才看到 :)
20:03 < hmchen> 剛剛看文件寫的, 是說當資料量大到 {單一 node + 多 GPU} 無法處理時, 搭配 mpi變成 {多node + 多 GPU)
20:04 < hmchen> 不過很難想像學校裡會開發出這種等級的程式, 論計算效能差不多是國防等級了
20:06 < hmchen> 我自己去看程式碼的感覺是, mpi 程式開發比 cuda 困難多了
20:07 < hmchen> 而 GPU 的浮點運算效能進展速度, 比CPU快的多, 也便宜的多
20:08 < hmchen> 如果沒有特殊考量, 直接用 cuda 比較快, 因為程式設計跟除錯的時間也要算進去
20:08 < guildwar> ok,了解，就看 haroldwu 要投資多少心力了 :)
20:09 < guildwar> 不過記得要買 nvidia 的顯卡 XD
20:10 < hmchen> 如果真的是學校要用的, 顯示卡算小錢
20:11 < hmchen> 也不用買到頂級的卡
20:12 < hmchen> 其實用 gpu 也有好處, 廠商為了推廣 GPGPU 技術, 都有開發 optimized libraray (fft,線性代數,矩陣運算,...)
20:22 < hmchen> http://kheresy.wordpress.com/2006/06/09/%E7%B0%A1%E6%98%93%E7%9A%84%E7%A8%8B%E5%BC%8F%E5%B9%B3%E8%A1%8C%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%8Dopenmp%EF%BC%88%E4%B8%80%EF%BC%89%E7%B0%A1%E4%BB%8B/
20:23 < hmchen> openmp, 簡單的平行化程式開發
20:23 < hmchen> 不過應該是單機多核心環境適用
20:28 < guildwar> 可是剛剛我 post 的文章中，有提到 openmp 是有支援 cuda-aware 的 mpi
20:28 < hmchen> 傳統 MPI + GPU 混合設計, 是利用 MPI 在傳送到 host memory, 然後再呼叫 cudaMemcopy 把資料搬到 GPU
20:29 < hmchen> cuda-aware MPI 語法則是讓 MPI 程式直接抓取 gpu 中的資料, 簡單來說就是把 cudaMemcpy 藏起來
20:30 < hmchen> 因為網路卡 driver 不可能讀取到顯示卡中的 memory, 還是得搬到 host memory
20:30 < hmchen> 這樣 MPI 才能用網路傳資料到其他 node
20:31 < hmchen> 所以應該是簡化程式設計的 MPI 版本
20:32 < hmchen> 當年也是因為論文的模擬程式要跑很久才看 MPI, 看一看就放棄了 XD
20:33 < guildwar> 所以應該學 openmpi 就好，不管 cpu 或 gpu 通吃 XD
20:33 < hmchen> 畢業後看到我用的模擬程式居然出了 cuda 版
20:33 < hmchen> opencl
20:33 < hmchen> 最終 opencl 還是會勝出
20:33 < hmchen> 雖然語法很囉唆
20:34 < hmchen> AMD顯卡上的 core 比較多, 玩遊戲不一定強, 但跑程式贏面就大多了, 但只能用 opencl 開發
20:35 < guildwar> 對啊，學 opencl 才是王道，不過相對要花更多時間
20:35 < hmchen> 後來 intel 內建 GPU 的 HD3000, HD4000 也開始支援 opencl
20:36 < hmchen> 行動裝置的 PowerVR SGX5xx 也有 opencl
20:36 < hmchen> 普及率真的很高
20:59 <@haroldwu> hmchen: 你猜對了，是同一個程式用不同的參數下去跑
21:00 <@haroldwu> 不過這裏有個問題啦，我其實是要跑蛋白質摺疊的，他是做趨近的動作，例如我參數設 10000，他就挑分數最高的 10000 個下去做趨近
21:01 <@haroldwu> 現在的問題是我可能得改 code 或啥的讓他這 10000 個任務併發，而且平臺也支援讓他併發運算
21:03 < guildwar> 平台 rocks 就應該沒問題了
21:07 < guildwar> 有個時間差異比較， "單cpu", "openmp 4cpu", "opencl 4cpu", "opencl amd gpu"
21:07 < guildwar> http://streamcomputing.eu/blog/2013-12-20/simple-sum-cpu-openmp-opencl-gpu/
21:07 < guildwar> 沒想到 openmp 4cpu 和 opencl 4cpu 差這麼多, 97.672 -> 27.325 秒
21:09 <@haroldwu> 那些機器都沒有 GPU 啦 XD
21:10 < guildwar> 內建顯卡是哪一張，搞不好直接就支援了
21:15 < guildwar> 那看看你的 cpu 有沒有支援 AVE 指令集，有的話可能要用 opencl 比較快
21:15 < guildwar> https://en.wikipedia.org/wiki/Advanced_Vector_Extensions
21:16 < guildwar> 錯了，不是 AVE，是 AVX
21:17 < guildwar> 剛才那個 97 -> 27 秒就是差在 AVX 指令集
21:26 <@haroldwu> 我現在沒辦法碰到機器 orz 要從實驗室走 VPN
21:27 < guildwar> 看了一下，openmp 4.0 也支援 AMD FirePro™ S9150 server card 了
21:27 < guildwar> http://ir.amd.com/phoenix.zhtml?c=74093&p=irol-newsArticle&ID=1955814
