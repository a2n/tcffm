--- Log opened 四 10月 30 00:00:34 2014
13:20 < guildwar> 要跑平行程式嗎? 若是要只要方便管理，建議直接用 rocks，還有其他選擇，但我只用過 rocks 就是
13:20 < guildwar> http://www.rocksclusters.org/wordpress/
13:24 < guildwar> 基本上設定好第一台 frontend node, 可再額外設幾台 storage nodes, 其他的都當做 compute nodes
13:29 < guildwar> 當然要自己從頭搞也可以，不過就工程浩大了
13:32 < guildwar> 不然用 drbl server 也行，不過要手動的地方比較多
13:50 < guildwar> 這裡有篇鳥哥用 centos 6.3 和 drbl 架設的 pc-cluster 可參考看看
13:50 < guildwar> http://linux.vbird.org/somepaper/20130521-build_pc_cluster_using_drbl_in_centos_6.3.pdf
14:53 <@haroldwu> 好
15:01 <@haroldwu> 我來研究看看，就是不知道 cluster 建起來以後，計算任務也要能分散到各個 node 才行啊 XDD
15:08 < guildwar> 那是你 coding 的問題，程式裡要改寫成呼叫 mpi 或 cuda 之類的，才會在跑時同時由多台一起跑
15:11 < guildwar> 若是程式不想改，那只能改用 ssi 的方式，不過通常這種方式不能太多台，可能頂多４台而已
15:11 < guildwar> http://kerrighed.org/wiki/index.php/Main_Page
15:41 < guildwar> 如果沒寫過 mpi 程式，這個應該可以參考 http://www.linux-mag.com/id/5759/
15:42 < guildwar> http://www.linux-mag.com/id/4609/
17:40 < dclee9> 今天的TC4H會參加台中前端社群定時聚會
17:40 < dclee9> 在夢種子
17:41 < dclee9> 所以如果有興趣的可以參加
17:41 < dclee9> 他們是講師型的聚會。
19:13 < hmchen> haroldwu 或許是要找 job scheduling 工具, http://en.wikipedia.org/wiki/Portable_Batch_System
19:15 < hmchen> 有時候只是有大量的程式要跑, 但不一定是平行運算, 我之前念研究所跑實驗數據, 是執行同一個程式但需要重複跑30次, 因為要求統計數據
19:19 < hmchen> 當時所裡有一套 PC cluster, 16 rocks x dual xeon, 上面安裝了一套叫 PBSPRo 的管理程式, 其中一個 node 跑 NFS server 掛 /home folder, 或者甚至是 root over NFS 開機, 有點忘了
19:21 < hmchen> 然後寫一個簡單的 shell script, 設定執行擋路徑, 需要幾個 nodes, 然後丟給 PBS 管理, 只要任何 compute node 有空, 就會把 job 自動丟進去跑
19:23 < hmchen> PBS Pro 要錢, 後來有自己試著用 Sun Grid Engine 架出相同功能的環境, 不是 open source 但不用錢, 現在變成 Oracle Grid Engine 了
19:24 < hmchen> 剛剛找一下,現在有類似的 projects, 有 Torque 跟 open grid engine
19:29 < hmchen> 如果是要跑單一任務的平行運算程式, 這年頭用 PC cluster + MPI 架構不太划算, 改學 cuda 或 OpenCL 搭配顯示卡 GPU, 說不定程式開發跟實際執行速度還比較快
19:31 < hmchen> 網路版的MPI平行化程式光是要除錯就會出人命了, cuda + gpu 起碼還是在單機上跑, 有 tool 可以即時除錯, 監看變數或中斷點之類的
19:36 < guildwar> 可是一開始就說硬體是 10 台機器囉，當然用 mpi 最合用
19:38 < guildwar> 學生不大可能去買一張幾萬元的 GPU，然後買 10 張
19:53 < hmchen> 還是看用途啦, 平行計算當然是 mpi 或 cuda, 多重任務程式就靠 pbs 這類的
19:55 < hmchen> 但說真的 GPU 比 MPI 強大多了, 硬體成本也比較便宜, 中階的卡其實就可以達到 4x~10x 不等的加速效果, 看計算架構而定
19:55 < hmchen> 因為 MPI 平行計算的瓶頸, 其實是卡在 data 的分散與再蒐集
19:56 < hmchen> node 之間得靠高速網路連接減少資料交換的等待時間
19:57 < hmchen> 以前沒有 gigabit ethernet 時, 還得用特殊網路卡
19:57 < hmchen> 窮人的 cluster 則是用多網卡 + channel bounding 倍增流量
19:57 < guildwar> 有看到這個，若 nvidia GPU 在機器上，就可以用 CUDA-Aware MPI
19:57 < guildwar> http://devblogs.nvidia.com/parallelforall/introduction-cuda-aware-mpi/
19:57 < hmchen> 但 GPU 跟 CPU 之間的頻寬遠大於網路卡
19:58 < guildwar> 畢竟要用 opencl 應該難度還是滿高的吧，光 porting code 就很慘了
19:59 < hmchen> cuda 語法比 opencl 簡單
19:59 < hmchen> 可惜就是 nvidia 限定
20:00 < hmchen> opencl 則可以 cpu + gpu 同時混合執行
20:00 < guildwar> 對啊，所以用 mpi 比較快上手
20:00 < guildwar> How to Build a GPU-Accelerated Research Cluster
20:00 < guildwar> http://devblogs.nvidia.com/parallelforall/how-build-gpu-accelerated-research-cluster/
20:00 < guildwar> Tune GPU and CPU load balancing for your application
20:01 < hmchen> 這個是把 cuda/opencl 包裝成 mpi 的架構去寫程式嗎?
20:01 < guildwar> 這個 cuda-aware mpi 好像就會自動分配 cpu 及 gpu
20:01 < guildwar> 我不懂，剛剛才看到 :)
20:03 < hmchen> 剛剛看文件寫的, 是說當資料量大到 {單一 node + 多 GPU} 無法處理時, 搭配 mpi變成 {多node + 多 GPU)
20:04 < hmchen> 不過很難想像學校裡會開發出這種等級的程式, 論計算效能差不多是國防等級了
20:06 < hmchen> 我自己去看程式碼的感覺是, mpi 程式開發比 cuda 困難多了
20:07 < hmchen> 而 GPU 的浮點運算效能進展速度, 比CPU快的多, 也便宜的多
20:08 < hmchen> 如果沒有特殊考量, 直接用 cuda 比較快, 因為程式設計跟除錯的時間也要算進去
20:08 < guildwar> ok,了解，就看 haroldwu 要投資多少心力了 :)
20:09 < guildwar> 不過記得要買 nvidia 的顯卡 XD
20:10 < hmchen> 如果真的是學校要用的, 顯示卡算小錢
20:11 < hmchen> 也不用買到頂級的卡
